<application>
  <component name="AppStorage">
    <histories>
      <item value="``nx`` forces ZADD to only create new elements and not to update scores for elements that already exist." />
      <item value="Set any number of element-name, score pairs to the key ``name``. Pairs are specified as a dict of element-names keys to score values. ``nx`` forces ZADD to only create new elements and not to update scores for elements that already exist. ``xx`` forces ZADD to only update scores of elements that already exist. New elements will not be added. ``ch`` modifies the return value to be the numbers of elements changed. Changed elements include new elements that were added and elements whose scores changed. ``incr`` modifies ZADD to behave like ZINCRBY. In this mode only a single element/score pair can be specified and the score is the amount the existing score will be incremented by. When using this mode the return value of ZADD will be the new score of the element. The return value of ZADD varies based on the mode specified. With no options, ZADD returns the number of new elements added to the sorted set." />
      <item value="nx" />
      <item value="mapping" />
      <item value="Read the response from a previously sent command" />
      <item value="options" />
      <item value="pieces" />
      <item value="iteritems" />
      <item value="get data store type" />
      <item value="Redis Stats Server" />
      <item value="sms alert" />
      <item value="Data Store" />
      <item value="extract" />
      <item value="weibo blocks" />
      <item value="Single" />
      <item value="attach column" />
      <item value="prepare process" />
      <item value="snapshot" />
      <item value="extensions" />
      <item value="logstats" />
      <item value="gnore" />
      <item value="TypeError: can only concatenate list (not &quot;dict&quot;) to list" />
      <item value="TypeError: a bytes-like object is required, not 'str'" />
      <item value="IndexError: pop from empty list" />
      <item value="NFO: Received SIG_SETMASK, shutting down gracefully. Send again to force" />
      <item value="SIG SETMASK" />
      <item value="denied" />
      <item value="Permission" />
      <item value="Default server work@192.168.129.17:22 has no valid mappings.&#10;Automatic upload is not enabled." />
      <item value="Enable this inspection if you need your code to be compatible with a range of Python versions (for example, if you're building a library). The range of Python versions with which the code needs to be compatible can be specified in the inspection settings." />
      <item value="Redis Spider" />
      <item value="getattr" />
      <item value="We allow optional crawler argument to keep backwards" />
      <item value="Raise a deprecation warning" />
      <item value="compatibility" />
      <item value="setup redis" />
      <item value="Setup redis connection and idle signal" />
      <item value="Returns a batch of start requests from redis" />
      <item value="get return value" />
      <item value=":param inputFormatClass: fully qualified classname of Hadoop InputFormat (e.g. &quot;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&quot;) :param keyClass: fully qualified classname of key Writable class (e.g. &quot;org.apache.hadoop.io.Text&quot;) :param valueClass: fully qualified classname of value Writable class (e.g. &quot;org.apache.hadoop.io.LongWritable&quot;) :param keyConverter: (None by default) :param valueConverter: (None by default) :param conf: Hadoop configuration, passed in as a dict (None by default) :param batchSize: The number of Python objects represented as a single Java object. (default 0, choose batchSize automatically)" />
      <item value="Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict. This will be converted into a Configuration in Java. The mechanism is the same as for sc.sequenceFile." />
      <item value="Exception: Java gateway process exited before sending its port number" />
      <item value="py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.&#10;: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.io.ImmutableBytesWritable" />
      <item value="wait result only if non-oneway" />
      <item value="thriftpy.thrift.T Application Exception" />
      <item value="stop Row" />
      <item value="start Row" />
      <item value="Any timestamps in the columns are ignored, use timeRange to select by timestamp. Max versions defaults to 1. Attributes: - startRow - stopRow - columns - caching - maxVersions - timeRange - filterString - batchSize - attributes - authorizations - reversed" />
      <item value="Grabs multiple rows from a Scanner. @return Between zero and numRows TResults Parameters: - scannerId: the Id of the Scanner to return rows from. This is an Id returned from the openScanner function. - numRows: number of rows to return" />
      <item value="Given a table and a row get the location of the region that would contain the given row key. reload = true means the cache will be cleared and the location will be fetched from meta. Parameters: - table - row - reload" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="120" />
        <entry key="ENGLISH" value="121" />
        <entry key="DANISH" value="1" />
        <entry key="LATVIAN" value="1" />
        <entry key="SWEDISH" value="1" />
        <entry key="ITALIAN" value="1" />
      </map>
    </option>
  </component>
</application>