<application>
  <component name="AppStorage">
    <histories>
      <item value="extract" />
      <item value="weibo blocks" />
      <item value="Single" />
      <item value="attach column" />
      <item value="prepare process" />
      <item value="snapshot" />
      <item value="extensions" />
      <item value="logstats" />
      <item value="gnore" />
      <item value="TypeError: can only concatenate list (not &quot;dict&quot;) to list" />
      <item value="TypeError: a bytes-like object is required, not 'str'" />
      <item value="IndexError: pop from empty list" />
      <item value="NFO: Received SIG_SETMASK, shutting down gracefully. Send again to force" />
      <item value="SIG SETMASK" />
      <item value="denied" />
      <item value="Permission" />
      <item value="Default server work@192.168.129.17:22 has no valid mappings.&#10;Automatic upload is not enabled." />
      <item value="Enable this inspection if you need your code to be compatible with a range of Python versions (for example, if you're building a library). The range of Python versions with which the code needs to be compatible can be specified in the inspection settings." />
      <item value="Redis Spider" />
      <item value="getattr" />
      <item value="We allow optional crawler argument to keep backwards" />
      <item value="Raise a deprecation warning" />
      <item value="compatibility" />
      <item value="setup redis" />
      <item value="Setup redis connection and idle signal" />
      <item value="Returns a batch of start requests from redis" />
      <item value="get return value" />
      <item value=":param inputFormatClass: fully qualified classname of Hadoop InputFormat (e.g. &quot;org.apache.hadoop.mapreduce.lib.input.TextInputFormat&quot;) :param keyClass: fully qualified classname of key Writable class (e.g. &quot;org.apache.hadoop.io.Text&quot;) :param valueClass: fully qualified classname of value Writable class (e.g. &quot;org.apache.hadoop.io.LongWritable&quot;) :param keyConverter: (None by default) :param valueConverter: (None by default) :param conf: Hadoop configuration, passed in as a dict (None by default) :param batchSize: The number of Python objects represented as a single Java object. (default 0, choose batchSize automatically)" />
      <item value="Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary Hadoop configuration, which is passed in as a Python dict. This will be converted into a Configuration in Java. The mechanism is the same as for sc.sequenceFile." />
      <item value="Exception: Java gateway process exited before sending its port number" />
      <item value="py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.&#10;: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.io.ImmutableBytesWritable" />
      <item value="wait result only if non-oneway" />
      <item value="thriftpy.thrift.T Application Exception" />
      <item value="stop Row" />
      <item value="start Row" />
      <item value="Any timestamps in the columns are ignored, use timeRange to select by timestamp. Max versions defaults to 1. Attributes: - startRow - stopRow - columns - caching - maxVersions - timeRange - filterString - batchSize - attributes - authorizations - reversed" />
      <item value="Grabs multiple rows from a Scanner. @return Between zero and numRows TResults Parameters: - scannerId: the Id of the Scanner to return rows from. This is an Id returned from the openScanner function. - numRows: number of rows to return" />
      <item value="Given a table and a row get the location of the region that would contain the given row key. reload = true means the cache will be cleared and the location will be fetched from meta. Parameters: - table - row - reload" />
      <item value="@return between zero and numRows TResults Parameters: - table: the table to get the Scanner for - tscan: the scan object to get a Scanner for - numRows: number of rows to return" />
      <item value="Get results for the provided TScan object. This helper function opens a scanner, get the results and close the scanner. @return between zero and numRows TResults Parameters: - table: the table to get the Scanner for - tscan: the scan object to get a Scanner for - numRows: number of rows to return" />
      <item value="Closes the scanner. Should be called to free server side resources timely. Typically close once the scanner is not needed anymore, i.e. after looping over it to get all the required rows. Parameters: - scannerId: the Id of the Scanner to close *" />
      <item value="Get a Scanner for the provided TScan object. @return Scanner Id to be used with other scanner procedures Parameters: - table: the table to get the Scanner for - tscan: the scan object to get a Scanner for" />
      <item value="interval count" />
      <item value="The primary :class:`pykafka.handlers.RequestHandler` for this broker This handler handles all requests outside of the commit/fetch api" />
      <item value="property" />
      <item value="produce_request: a request object indicating the messages to produce" />
      <item value="function accepting a :class:`pykafka.broker.Broker` as its sole argument that returns a :class:`pykafka.protocol.Response`. The argument to this function will be the each of the brokers discoverable via `broker_connects` in turn." />
      <item value="ake a request to any broker in broker_connects Returns the result of the first successful request :param broker_connects: The set of brokers to which to attempt to connect :type broker_connects: Iterable of two-element sequences of the format (broker_host, broker_port) :param req_fn: A function accepting a :class:`pykafka.broker.Broker` as its sole argument that returns a :class:`pykafka.protocol.Response`. The argument to this function will be the each of the brokers discoverable via `broker_connects` in turn. :type req_fn: function" />
      <item value="Establish a connection to the broker server. Creates a new :class:`pykafka.connection.BrokerConnection` and a new :class:`pykafka.handlers.RequestHandler` for this broker" />
      <item value="nsures that self._req_handler is not None before calling fn" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="105" />
        <entry key="ENGLISH" value="106" />
        <entry key="LATVIAN" value="1" />
        <entry key="SWEDISH" value="1" />
        <entry key="ITALIAN" value="1" />
      </map>
    </option>
  </component>
</application>