<application>
  <component name="AppStorage">
    <histories>
      <item value="The primary :class:`pykafka.handlers.RequestHandler` for this broker This handler handles all requests outside of the commit/fetch api" />
      <item value="property" />
      <item value="produce_request: a request object indicating the messages to produce" />
      <item value="function accepting a :class:`pykafka.broker.Broker` as its sole argument that returns a :class:`pykafka.protocol.Response`. The argument to this function will be the each of the brokers discoverable via `broker_connects` in turn." />
      <item value="ake a request to any broker in broker_connects Returns the result of the first successful request :param broker_connects: The set of brokers to which to attempt to connect :type broker_connects: Iterable of two-element sequences of the format (broker_host, broker_port) :param req_fn: A function accepting a :class:`pykafka.broker.Broker` as its sole argument that returns a :class:`pykafka.protocol.Response`. The argument to this function will be the each of the brokers discoverable via `broker_connects` in turn. :type req_fn: function" />
      <item value="Establish a connection to the broker server. Creates a new :class:`pykafka.connection.BrokerConnection` and a new :class:`pykafka.handlers.RequestHandler` for this broker" />
      <item value="nsures that self._req_handler is not None before calling fn" />
      <item value="Socket disconnected during request for &quot; &quot;broker %s:%s. Continuing" />
      <item value="Make a request to any broker in broker_connects Returns the result of the first successful request :param broker_connects: The set of brokers to which to attempt to connect :type broker_connects: Iterable of two-element sequences of the format (broker_host, broker_port) :param req_fn: A function accepting a :class:`pykafka.broker.Broker` as its sole argument that returns a :class:`pykafka.protocol.Response`. The argument to this function will be the each of the brokers discoverable via `broker_connects` in turn. :type req_fn: function" />
      <item value="Get a list of host:port pairs representing possible broker connections For use only when self.brokers is not populated (ie at startup)" />
      <item value="pages" />
      <item value="Crawled" />
      <item value="scope" />
      <item value="Receive" />
      <item value="Please switch to the latest stable if merging failed." />
      <item value="It seems that your ffmpeg is a nightly build" />
      <item value="upload file to dfs failure,path" />
      <item value="can not find path" />
      <item value="accept unknow value" />
      <item value="terminate" />
      <item value="url in queue now" />
      <item value="Unexpected" />
      <item value="full_search_list count is" />
      <item value="Ready to Add words for" />
      <item value="urls are waiting" />
      <item value="Crawl slow" />
      <item value="Now we have" />
      <item value="will be inserted" />
      <item value="compare with words exist" />
      <item value="freq" />
      <item value="freq words" />
      <item value="We get" />
      <item value="We get {update} {level} freq words,' 'compare with words exist,' '{insert} will be inserted,' '{delete} will be deleted.' 'Now we have {count} word" />
      <item value="local variable 'FDFS_CLIENT' referenced before assignment" />
      <item value="extract the snap for fastdfs, which should not be inserted into hbase. :param item: :return: the bytes of snap." />
      <item value="attach column" />
      <item value="attach columns which could be get from existing column. This method depend on different info type. :param item: :return:" />
      <item value="batch check by boom filter. :param check_list: :return:the data not in the filter." />
      <item value="TypeError: analysis() missing 1 required positional argument: 'meta'" />
      <item value="Use redis pipeline execution" />
      <item value="Returns a request to be scheduled or none." />
      <item value="encoding" />
      <item value="batch size" />
      <item value="Reading start URLs from redis ke" />
      <item value="redis batch size" />
      <item value="redis_key must not be empty" />
      <item value="e allow optional crawler argument to keep backwards # compatibility. # XXX: Raise a deprecation warning." />
      <item value="Setup redis connection and idle signal. This should be called after the spider has set its crawler object." />
      <item value="Returns a batch of start requests from redis" />
      <item value="Mixin class to implement reading urls from a redis queue&quot;" />
    </histories>
    <option name="languageScores">
      <map>
        <entry key="CHINESE" value="60" />
        <entry key="ENGLISH" value="61" />
      </map>
    </option>
  </component>
</application>